import os
import sys
import json
import rlcompleter, readline
readline.parse_and_bind('tab:complete')

__jlogfmt = {
  'asctime': '%(asctime)s',
  'thread': '%(threadName)-12.12s',
  'level': '%(levelname)-5.5s',
  'module': '%(module)s',
  'filename': '%(filename)s:%(lineno)d',
  'func': '%(funcName)s',
  'msg': '%(message)s'
}

if os.getenv('PYDEBUG'):
    import logging
    from logging.handlers import SocketHandler
    from uuid import uuid4

    # path = 'unix:///tmp/pylog-' + str(uuid4()).split('-')[0]
    path = f'/tmp/pylog-{os.getenv("PYDEBUG")}'
    print(f'sock path: {path}')
    logging.basicConfig(
        level=logging.DEBUG,
        format=json.dumps(__jlogfmt),
        handlers=[
            SocketHandler(path, None)
        ])
    logging.getLogger("parso").setLevel(logging.ERROR)
    logging.getLogger(__name__).debug(f'debug logging enabled ({path})')


def logdebug():
    import logging
    logging.basicConfig(level=logging.DEBUG)
    logging.getLogger("parso").setLevel(logging.ERROR)
    logging.getLogger(__name__).debug('debug logging enabled')

class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        from uuid import UUID
        from datetime import datetime
        if isinstance(obj, (UUID, datetime)):
            return str(obj)
        if isinstance(obj, set):
            return list(obj)
        if hasattr(obj, '__json__'):
            return obj.__json__()
        return json.JSONEncoder.default(self, obj)

def jdump(d, **kwargs):
    import json
    if isinstance(d, str):
        d = json.loads(d)
    return json.dumps(d, cls=CustomJSONEncoder, **kwargs)

def jwrite(d, path):
    with open(path, 'w') as of:
        of.write(jdump(d, indent=2, sort_keys=True))

def jread(path):
    import json
    with open(path) as of:
        return json.loads(of.read())

def jprint(d):
    import json
    from collections.abc import KeysView, ValuesView

    if isinstance(d, str):
        d = json.loads(d)

    if isinstance(d, (KeysView, ValuesView, set)):
        d = list(d)

    print(jdump(d, indent=2, sort_keys=True))

def csvread(path):
    import csv
    idx, rows = 0, []
    with open(path) as of:
        try:
            for row in csv.reader(of, quotechar='"'):
                rows.append(row)
                idx+=1
        except Exception as ex:
            print('error at row {idx}')
            raise ex
    return rows

def lprint(l):
    for idx, v in enumerate(l):
        print(f'{idx}: {v}')

def fcount(items, key):
    # count unique values for field in list of dicts
    from collections import defaultdict
    counter = defaultdict(int)
    for x in items:
        if key in x:
            counter[x[key]] += 1
        else:
            counter['MISSING_FIELD'] += 1
    jprint(counter)

def CamelCase2lower_case(s):
    chars = [ x for x in s ]
    newname = chars.pop(0).lower()
    for c in chars:
        if c.isupper():
            newname += '_'
            newname += c.lower()
        else:
            newname += c
    return newname

def stddev(nset):
    import math
    mean = sum(nset) / len(nset)
    vset = [ (x - mean)**2 for x in nset ]
    sd = math.sqrt(sum(vset) / (len(nset) - 1))
    print('sd     : %s' % sd)
    print('min sd : %s' % (mean - sd))
    print('max sd : %s' % (mean + sd))

def getsoup(url):
    import requests
    from bs4 import BeautifulSoup
    r = requests.get(url)
    r.raise_for_status()
    return BeautifulSoup(r.text, 'html.parser')

def rdl(url, fname=None):
    import requests
    if not fname:
        fname = url.split('/')[-1]
    with requests.get(url, stream=True) as r:
        with open(fname, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return fname

def chromeopen(url):
    from subprocess import call
    call(['google-chrome-stable', '--incognito', url])

def iterfiles(d, maxdepth=None):
    import os

    depth = lambda x: x.strip('./').count('/') + 1 # get depth from root path

    for root, dirs, files in os.walk(d):
        for fname in files:
            if maxdepth and depth(root) > maxdepth:
                break
            yield os.path.join(root, fname)

def truncate(n, decimals=0):
    """ truncate float at given decimal (round down) """
    multiplier = 10 ** decimals
    return int(n * multiplier) / multiplier

def yread(path):
    import yaml
    with open(path) as of:
        d = list(yaml.load_all(of.read(), Loader=yaml.FullLoader))
    if len(d) == 1:
        return d[0]
    return d

def yaml2json(path):
    import yaml
    with open(path) as of:
        d = yaml.load_all(of.read(), Loader=yaml.FullLoader)
    jprint(list(d))

def pivot(items, key):
    from collections import defaultdict
    res = defaultdict(list)
    for x in items:
        res[x.get(key)].append(x)
    return res

def escape_ansi(line):
    import re
    ansi_escape = re.compile(r'(\x9B|\x1B\[)[0-?]*[ -/]*[@-~]')
    return ansi_escape.sub('', line)

# column-aligned output
def column(items, spacing=2):
    ret = []
    col_w = [ 1 for _ in items[0] ] # default minimum widths

    _len = lambda x: len(escape_ansi(x))

    def _maxw(line):
        for n, txt in enumerate(line):
            if _len(txt) > col_w[n]:
                col_w[n] = _len(txt)

    for line in items:
        _maxw(line)
    col_w = [ w+spacing for w in col_w ]

    for line in items:
        ret.append(''.join([ f'{txt: <{w}}' for w, txt in zip(col_w, line) ]))

    return '\n'.join(ret)

def __md2html_toc(body):
    """
    inject a toc into md-generated html body, if any
    and return new body string
    """
    tags = ('{toc}', '{:toc}')

    if not body.toc_html:
        return body

    span = [ body.find(tags[0]), body.find(tags[1]) ]
    if span[0] == -1 and span[1] == -1:
        toc = f'<div class="toc">\n{body.toc_html}\n</div>\n'
        return toc + body

    if span[0] == -1:
        toc = f'<div class="toc">\n{body.toc_html}\n</div>\n'
        return body.replace(tags[1], toc)

    # extract elements within tags
    toc_right = body[span[0]+len(tags[0]):span[1]]

    # prepare for mutation
    span[1] += len(tags[1])

    # strip leading / trailing paragraph, if any
    if body[span[0] - 3:span[0]] == '<p>':
        span[0] -= 3
    if body[span[1]:span[1]+4] == '</p>':
        span[1] += 4

    return '\n'.join([
      body[:span[0]],
      '<div class="toc">',
      f'<div class="toc-left">\n{body.toc_html}\n</div>',
      f'<div class="toc-right"><p>{toc_right}</p></div>',
      '</div>',
      body[span[1]:],
    ])

def md2html(path, target=None):
    from markdown2 import markdown, markdown_path

    if not target:
        target = path.replace('.md', '.html')

    ssheet = '/home/bradley/repos/pygments-css/friendly.css'

    extras = {
      'toc': {},
      'fenced-code-blocks': {'linenos': False},
      # 'highlightjs-lang': {},
      'tables': {},
      # 'code-friendly': None,
      'html-classes': {
          'pre': 'quote',
          'code': 'ocode',
          'table': 'pretty-table'
      },
    }

    doc = ''
    with open(ssheet) as of:
        doc += '<style>\n' + of.read() + '\n</style>\n'

    body = markdown_path(path, extras=extras)
    body = __md2html_toc(body)

    doc += f'<div class="container">\n{body}\n</div>'

    with open(target, 'w') as of:
        n = of.write(doc)
        print(f'wrote {n}b to {target}')

def _walkfiles(path):
    for root, _, files in os.walk(path):
        for f in files: yield os.path.abspath(os.path.join(root, f))

def _pyfiles(path):
    return filter(lambda x: x.endswith('.py'), _walkfiles(path))

def ast_imports(path):
    import os

    if os.path.isdir(path):
        for fp in _pyfiles(path):
            yield from __ast_imports(fp)
    else:
        yield from __ast_imports(path)

def __ast_imports(path):
    import ast
    from collections import namedtuple

    Import = namedtuple("Import", ["module", "name", "alias", "file"])

    try:
        with open(path) as fh:
           root = ast.parse(fh.read(), path)
    except Exception as ex:
        print(f'failed to parse {path}: {ex}')
        return

    for node in ast.iter_child_nodes(root):
        if isinstance(node, ast.Import):
            module = ''
        elif isinstance(node, ast.ImportFrom):
            module = node.module
        else:
            continue

        for n in node.names:
            yield Import(module, n.name, n.asname, path)

class DictStats(dict):

    def add(self, *dicts):
        for d in dicts:
            self._add(d)

    def unique_kv_counts(self):
        rows = []
        for k,v in self.items():
            rows.append([ str(k), len(v) ])
        rows = [ [x[0], str(x[1])] for x in
                sorted(rows, key=lambda x: x[1], reverse=True) ]
        print(column(rows))

    def _add(self, d):
        from collections import defaultdict
        for k,v in d.items():
            if not isinstance(v, (str,int,float,bool)):
                continue
            if k not in self:
                self[k] = defaultdict(int)
            self[k][v] += 1

def epoch2dt(ts, tz=None):
    # parse and return a datetime.datetime from a
    # unix timestamp
    import pytz
    from datetime import datetime
    if not isinstance(ts, int):
        ts = int(ts)
    if ts > 16000000000:
        ts = ts / 1000

    dt = datetime.utcfromtimestamp(ts)
    dt = dt.astimezone(pytz.timezone('UTC'))
    if tz:
        dt = dt.astimezone(tz)
    return dt

if sys.argv[0].split('/')[-1] == '.pythonrc':
    if len(sys.argv) < 2:
        print('no function provided')
        sys.exit(1)
    fn = locals().get(sys.argv[1])
    if not fn:
        print(f'function not found: {sys.argv[1]}')
        sys.exit(1)
    fn(*sys.argv[2:])
